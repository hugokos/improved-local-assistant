# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=2
OLLAMA_REQUEST_TIMEOUT=120

# Model Configuration
CONVERSATION_MODEL=hermes3:3b
KNOWLEDGE_MODEL=tinyllama
MODEL_CONTEXT_WINDOW=8000

# System Configuration
MAX_MEMORY_GB=12
CPU_CORES=8
ENABLE_GPU=false

# Knowledge Graph Configuration
PREBUILT_GRAPHS_DIR=./data/prebuilt_graphs
DYNAMIC_GRAPH_STORAGE=./data/dynamic_graph
GRAPH_MAX_TRIPLETS_PER_CHUNK=4

# API Configuration
API_HOST=localhost
API_PORT=8000
WEBSOCKET_TIMEOUT=300

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=./logs/assistant.log
ENABLE_DEBUG_LOGGING=false

# Performance Configuration
RESPONSE_TIMEOUT_SECONDS=30
MAX_CONVERSATION_HISTORY=50
ENABLE_CONVERSATION_SUMMARIZATION=true