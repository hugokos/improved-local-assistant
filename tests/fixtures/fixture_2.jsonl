{"role": "user", "content": "TinyLlama keeps running out of memory on my Raspberry Pi 5."}
{"role": "assistant", "content": "Quantise to Q4_K_M or switch to Phi-3-mini; both fit inside 4 GB."}
{"role": "user", "content": "How do I quantise?"}
{"role": "assistant", "content": "Run 'ollama pull phi3:mini && llama-quantize –preset Q4_K_M'."}
{"role": "user", "content": "Do I need AVX-512 for that?"}
{"role": "assistant", "content": "No—Pi 5 is ARM; just compile llama.cpp with make LLAMA_NO_MULTITHREAD=0 for NEON optimizations."}
{"role": "user", "content": "What thread count is ideal?"}
{"role": "assistant", "content": "Set OLLAMA_NUM_PREDICT to 256 and use 8 threads; that saturates all A76 cores."}
{"role": "user", "content": "How can I monitor memory while it runs?"}
{"role": "assistant", "content": "Use psutil.Process().memory_percent(); alert if it exceeds 90 percent."}
{"role": "user", "content": "Great—how do I enable keep-alive caching?"}
{"role": "assistant", "content": "Export OLLAMA_KEEP_ALIVE=30 so the model stays warm for quick successive calls."}