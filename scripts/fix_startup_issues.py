#!/usr/bin/env python3
"""
Quick fix script for common startup issues.

This script addresses the most common startup problems:
1. Wrong embedding model ID
2. Ollama model listing issues
3. Windows process priority warnings
4. Missing dependencies
"""

import os
import sys
import subprocess
import platform
from pathlib import Path

def check_and_fix_embedding_model():
    """Check and fix embedding model configuration."""
    print("ğŸ” Checking embedding model configuration...")
    
    # Check if the correct model is configured
    config_file = Path("config.yaml")
    if config_file.exists():
        content = config_file.read_text()
        if "sentence-transformers/bge-small-en-v1.5" in content:
            print("âŒ Found incorrect embedding model ID")
            content = content.replace(
                "sentence-transformers/bge-small-en-v1.5", 
                "BAAI/bge-small-en-v1.5"
            )
            config_file.write_text(content)
            print("âœ… Fixed embedding model ID in config.yaml")
        elif "BAAI/bge-small-en-v1.5" in content:
            print("âœ… Embedding model ID is correct")
        else:
            print("âš ï¸  No embedding model configuration found")
    else:
        print("âš ï¸  config.yaml not found")

def check_ollama_connection():
    """Check Ollama connection and models."""
    print("\nğŸ” Checking Ollama connection...")
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json()
            available_models = []
            for m in models.get('models', []):
                model_name = m.get('model') or m.get('name', '')
                if model_name:
                    available_models.append(model_name)
            
            print(f"âœ… Ollama is running with {len(available_models)} models")
            print(f"ğŸ“‹ Available models: {', '.join(available_models)}")
            
            # Check required models (handle :latest suffix)
            required_models = ["hermes3:3b", "tinyllama"]
            available_base_models = [m.split(':')[0] for m in available_models]
            missing_models = []
            
            for required in required_models:
                base_required = required.split(':')[0]
                if base_required not in available_base_models and required not in available_models:
                    missing_models.append(required)
            
            if missing_models:
                print(f"âŒ Missing required models: {', '.join(missing_models)}")
                print("ğŸ’¡ Run: ollama pull hermes3:3b && ollama pull tinyllama")
            else:
                print("âœ… All required models are available")
                
        else:
            print(f"âŒ Ollama API returned status {response.status_code}")
    except requests.exceptions.ConnectionError:
        print("âŒ Cannot connect to Ollama (is it running?)")
        print("ğŸ’¡ Start Ollama with: ollama serve")
    except ImportError:
        print("âŒ requests library not available")
    except Exception as e:
        print(f"âŒ Error checking Ollama: {e}")

def check_dependencies():
    """Check and install missing dependencies."""
    print("\nğŸ” Checking Python dependencies...")
    
    required_packages = [
        "sentence-transformers",
        "torch",
        "transformers",
        "ollama",
        "fastapi",
        "uvicorn",
        "websockets"
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package.replace("-", "_"))
            print(f"âœ… {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"âŒ {package}")
    
    if missing_packages:
        print(f"\nğŸ’¡ Install missing packages: pip install {' '.join(missing_packages)}")
    else:
        print("\nâœ… All required packages are installed")

def fix_windows_issues():
    """Fix Windows-specific issues."""
    if platform.system() != "Windows":
        return
        
    print("\nğŸ” Checking Windows-specific issues...")
    
    # Check for win32api
    try:
        import win32api
        print("âœ… win32api is available")
    except ImportError:
        print("âš ï¸  win32api not available (process priority adjustment disabled)")
        print("ğŸ’¡ Install with: pip install pywin32")
    
    # Set environment variables for Windows
    os.environ["SKIP_PROCESS_PRIORITY"] = "1"
    os.environ["PYTHONIOENCODING"] = "utf-8"
    print("âœ… Set Windows environment variables")

def download_embedding_model():
    """Download embedding model for offline use."""
    print("\nğŸ” Checking embedding model availability...")
    
    try:
        from sentence_transformers import SentenceTransformer
        
        model_name = "BAAI/bge-small-en-v1.5"
        print(f"ğŸ“¥ Downloading {model_name}...")
        
        # This will download the model if not already cached
        model = SentenceTransformer(model_name, device="cpu")
        print("âœ… Embedding model is available")
        
        # Test the model
        test_embedding = model.encode("test sentence")
        print(f"âœ… Model test successful (embedding size: {len(test_embedding)})")
        
    except Exception as e:
        print(f"âŒ Error with embedding model: {e}")
        print("ğŸ’¡ Try: pip install sentence-transformers torch")

def create_directories():
    """Create necessary directories."""
    print("\nğŸ” Creating necessary directories...")
    
    directories = [
        "logs",
        "data",
        "data/prebuilt_graphs",
        "data/dynamic_graph", 
        "data/sessions"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"âœ… {directory}")

def main():
    """Run all fixes."""
    print("ğŸš€ Improved Local AI Assistant - Startup Issue Fixer")
    print("=" * 50)
    
    # Change to project directory
    script_dir = Path(__file__).parent
    project_dir = script_dir.parent
    os.chdir(project_dir)
    print(f"ğŸ“ Working directory: {project_dir}")
    
    # Run all checks and fixes
    check_and_fix_embedding_model()
    check_ollama_connection()
    check_dependencies()
    fix_windows_issues()
    download_embedding_model()
    create_directories()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ Startup issue fixes completed!")
    print("\nğŸ’¡ Now try running: python app.py --preload models --lazy-load-graphs")

if __name__ == "__main__":
    main()