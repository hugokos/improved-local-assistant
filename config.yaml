api:
  cors_origins:
  - '*'
  host: 0.0.0.0
  port: 8000
connection_pool:
  keep_alive_models:
  - phi3:mini  # Use smaller model that fits in available memory
  keepalive_expiry: 30.0
  max_connections: 10
  max_keepalive_connections: 5
  timeout:
    connect: 10.0  # increased from 5s
    pool: 10.0     # increased from 5s
    read: 300.0    # 5 minutes - increased from 30s
    write: 300.0   # 5 minutes - increased from 30s
conversation:
  context_window_tokens: 8000
  max_history_length: 50
  summarize_threshold: 20
edge_optimization:
  enabled: true
  mode: production
embeddings:
  type: local
  model: BAAI/bge-small-en-v1.5
  device: cpu
  int8_quantization: true
  singleton: true

llama_index:
  embed_model: local
environment: development
environment_iphone:
  api:
    host: localhost
    port: 3000
  models:
    conversation:
      context_window: 2000
      max_tokens: 512
    knowledge:
      context_window: 512
      max_tokens: 256
  server_env:
    OLLAMA_MAX_LOADED_MODELS: 1
    OLLAMA_NUM_PARALLEL: 1
  system:
    cpu_threshold_percent: 60
    max_memory_gb: 2
    memory_threshold_percent: 60
environment_production:
  api:
    cors_origins:
    - http://localhost:8000
  system:
    cpu_threshold_percent: 70
    memory_threshold_percent: 70
environment_raspberry_pi:
  models:
    conversation:
      context_window: 4000
      max_tokens: 1024
    knowledge:
      context_window: 1024
      max_tokens: 512
  server_env:
    OLLAMA_MAX_LOADED_MODELS: 1
    OLLAMA_NUM_PARALLEL: 1
  system:
    cpu_threshold_percent: 70
    max_memory_gb: 4
    memory_threshold_percent: 70
extraction:
  format: json
  input_window_turns: 3
  max_time_seconds: 60.0  # 1 minute - increased from 12s
  max_tokens: 1024
  max_triples_per_turn: 10
  skip_on_cpu_pressure: true
  skip_on_memory_pressure: true
  unload_after_extraction: true
hybrid_retriever:
  use_rrf: true
  half_life_secs: 604800  # 1 week
  rerank_top_n: 10
  budget:
    bm25_top_k: 4
    graph_depth: 2
    max_chunks: 12
    vector_top_k: 4
  query_fusion:
    enabled: true
    num_queries: 3
  response_mode: compact
  vector:
    device: cpu
    int8: true
    model: BAAI/bge-small-en-v1.5
  weights:
    bm25: 0.15
    graph: 0.6
    vector: 0.25
knowledge_graphs:
  chunk_size_tokens: 256
  dynamic_storage: ./data/dynamic_graph
  enable_caching: true
  enable_visualization: false
  max_triplets_per_chunk: 3
  prebuilt_directory: ./data/prebuilt_graphs

graph:
  type: property  # "simple" or "property"
  store: simple   # Use SimplePropertyGraphStore (in-memory)

dynamic_kg:
  episode_every_turns: 8
  persist_every_updates: 20
  persist_interval_secs: 300
models:
  conversation:
    context_window: 8000
    max_tokens: 2048
    name: hermes3:3b
    temperature: 0.7
    # Available options for UI toggle
    options:
      - name: hermes3:3b
        display_name: "Hermes 3:3B"
        context_window: 8000
        max_tokens: 2048
      - name: tinyllama
        display_name: "TinyLlama"
        context_window: 4000
        max_tokens: 1024
  knowledge:
    context_window: 4096
    max_tokens: 1024
    name: phi3:mini
    temperature: 0.2
    # Available options for UI toggle
    options:
      - name: tinyllama
        display_name: "TinyLlama"
        context_window: 2048
        max_tokens: 1024
      - name: phi3:mini
        display_name: "Phi-3 Mini"
        context_window: 4096
        max_tokens: 1024
ollama:
  host: http://localhost:11434
  max_loaded_models: 2
  max_parallel: 2
  timeout: 600  # 10 minutes - increased from 2 minutes
orchestration:
  extraction_skip_on_pressure: true
  json_mode_for_extraction: true
  keep_alive_hermes: 30m
  keep_alive_tinyllama: 0
  llm_semaphore_timeout: 120.0  # 2 minutes - increased from 30s
  max_extraction_time: 60.0  # 1 minute - increased from 8s
server_env:
  OLLAMA_MAX_LOADED_MODELS: 1
  OLLAMA_NUM_PARALLEL: 1
startup:
  lazy_load_graphs: true
  ollama_healthcheck: version
  preload: models
system:
  cpu_cores: 4
  cpu_threshold_percent: 90
  max_memory_gb: 12
  memory_threshold_percent: 95
  monitor_debounce: 90
  monitor_interval: 15
  startup_grace_period: 30
  quiet_monitoring: true  # Suppress resource usage warnings in CLI

# Memory fallback configuration
memory_fallback:
  enabled: true
  primary_model: hermes3:3b
  fallback_model: tinyllama
  proactive_threshold_percent: 98  # Switch to fallback when memory usage exceeds this
  error_patterns:
    - "model requires more system memory"
    - "500 Internal Server Error"
    - "out of memory"
    - "insufficient memory"
  auto_reset_after_minutes: 10
working_set_cache:
  eviction_threshold: 0.8
  global_memory_limit_mb: 256
  include_text: false
  max_edges_per_query: 40
  nodes_per_session: 100
  persist_dir: ./storage

# Voice processing configuration
voice:
  enabled: true
  stt:
    enabled: true
    model_name: small-en  # vosk-model-small-en-us
    model_path: ./models/vosk/vosk-model-small-en
    sample_rate: 16000
  tts:
    enabled: true
    voice_name: en_US-lessac-medium
    voice_path: ./models/piper/en_US-lessac-medium/en_US-lessac-medium.onnx
    sample_rate: 22050
    chunk_size: 1024
    speed: 1.0
  vad:
    enabled: true
    aggressiveness: 2  # 0-3, higher = more aggressive
    frame_duration_ms: 30  # 10, 20, or 30ms
    speech_threshold: 3  # Frames to confirm speech start
    silence_threshold: 10  # Frames to confirm speech end
