# Improved Local Assistant Configuration
# Main configuration file for the application

# API Configuration
api:
  host: localhost
  port: 8000
  cors_origins:
    - "*"

# Ollama Configuration
ollama:
  host: http://localhost:11434
  timeout: 600  # 10 minutes
  max_parallel: 2
  max_loaded_models: 2

# Model Configuration
models:
  conversation:
    name: hermes3:3b
    context_window: 8000
    max_tokens: 2048
    temperature: 0.7
    options:
      - name: hermes3:3b
        display_name: "Hermes 3:3B"
        context_window: 8000
        max_tokens: 2048
      - name: phi3:mini
        display_name: "Phi-3 Mini"
        context_window: 4096
        max_tokens: 1024
  knowledge:
    name: tinyllama:latest
    context_window: 4096
    max_tokens: 1024
    temperature: 0.2
    options:
      - name: tinyllama:latest
        display_name: "TinyLlama"
        context_window: 2048
        max_tokens: 1024
      - name: phi3:mini
        display_name: "Phi-3 Mini"
        context_window: 4096
        max_tokens: 1024

# Embedding Configuration
embedding:
  model_path: BAAI/bge-small-en-v1.5
  device: cpu
  singleton: true
  int8_quantization: true

# Knowledge Graph Configuration
knowledge_graphs:
  prebuilt_directory: ./data/prebuilt_graphs
  dynamic_storage: ./data/dynamic_graph
  chunk_size_tokens: 256
  max_triplets_per_chunk: 3
  enable_caching: true
  enable_visualization: true

# Voice Configuration
voice:
  enabled: true
  stt:
    provider: vosk
    model_path: ./models/vosk/vosk-model-small-en
    sample_rate: 16000
  tts:
    provider: piper
    model_path: ./models/piper/en_US-lessac-medium.onnx
    sample_rate: 22050
  vad:
    provider: webrtc
    aggressiveness: 2

# Voice Model Download Configuration
voice_models:
  auto_download: true
  vosk:
    model: small-en
    url: https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
  piper:
    model: en_US-lessac-medium
    url: https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/lessac/medium/en_US-lessac-medium.onnx

# Conversation Configuration
conversation:
  max_history_length: 50
  context_window_tokens: 8000
  summarize_threshold: 20

# System Configuration
system:
  max_memory_gb: 12
  memory_threshold_percent: 90
  cpu_threshold_percent: 90
  monitor_interval: 15
  monitor_debounce: 60
  quiet_monitoring: false

# Startup Configuration
startup:
  preload: models
  lazy_load_graphs: true
  ollama_healthcheck: version

# Connection Pool Configuration
connection_pool:
  max_connections: 10
  max_keepalive_connections: 5
  keepalive_expiry: 30.0
  keep_alive_models:
    - hermes3:3b
  timeout:
    connect: 10.0
    pool: 10.0
    read: 300.0
    write: 300.0

# Extraction Configuration
extraction:
  max_tokens: 1024
  max_triples_per_turn: 10
  max_time_seconds: 60.0
  format: json
  input_window_turns: 3
  skip_on_memory_pressure: true
  skip_on_cpu_pressure: true
  unload_after_extraction: true

# Hybrid Retriever Configuration
hybrid_retriever:
  budget:
    vector_top_k: 3
    bm25_top_k: 3
    graph_depth: 2
    max_chunks: 12
  weights:
    vector: 0.2
    bm25: 0.2
    graph: 0.6
  query_fusion:
    enabled: true
    num_queries: 3
  response_mode: compact
  vector:
    model: BAAI/bge-small-en-v1.5
    device: cpu
    int8: true

# Working Set Cache Configuration
working_set_cache:
  global_memory_limit_mb: 256
  nodes_per_session: 100
  max_edges_per_query: 40
  eviction_threshold: 0.8
  include_text: false
  persist_dir: ./storage

# Edge Optimization Configuration
edge_optimization:
  enabled: false
  mode: development

# Environment Configuration
environment: development